---
title: "Illumina fastq QC metrics"
output:
  html_document:
    highlight: tango
    theme: cosmo
    toc: no
params:
  files: NULL
---

Report generated on `r Sys.time()` by the [angelovangel/nxf-fastqc](https://github.com/angelovangel/nxf-fastqc) pipeline. All fastq data is calculated with the [faster](https://github.com/angelovangel/faster) and [fastkmers](https://github.com/angelovangel/fastkmers) programs.


```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE, 
                      echo = FALSE, 
                      warning = FALSE, 
                      cache = FALSE)
#require(seqTools)
require(stringr)
require(writexl)
require(knitr)
require(DT)
require(kableExtra)
require(dplyr)
require(sparkline)
require(htmlwidgets)
require(jsonlite)
require(parallel) # may be ships with R, so not in the environment.yml
#require(RcppRoll)
```

```{r faster}

stats_headers <- c("file", "num_seqs", "bases", "n_bases", 
									 "min_len", "max_len", "avg_len", "Q1", "Q2", "Q3", 
									 "N50", "Q20_percent", "Q30_percent")

fqfiles <- params$files

stats <- system2(command = "parallel", 
								 args = c("-k", "--will-cite", "faster", "-t", ":::", fqfiles), 
								 stdout = TRUE)


df <- stats[grep("^file", stats, invert = TRUE)] %>% # this can be removed if the -ts option in faster is used
	read.table(text = ., col.names = stats_headers) %>% 
	dplyr::mutate(file = basename(file)) %>% 
	dplyr::arrange(file) %>%
	dplyr::select(-c(8:11))

# the seqtools script lives here
# args are individual fastq.gz files

# AUX FUNCTIONS
#=======================================================================
# aux function needed for calc Q20% and Q30%
# phredDist from seqTools returns the phred score distribution, 
# so to get Q20% use get_q(qq, 21) because the vector is zero-based
# get_q <- function(qqobj, q) {
# 	round( sum( phredDist(qqobj)[q:length(phredDist(qqobj))] ) * 100, digits = 2)
# }

# aux function to get N50 or Nx from the qq object
# n is 0.5 for N50 etc...
# get_nx <- function(qqobj, n) {
# 	slc <- seqLenCount(qqobj)
# 	
# 	# get a vector with read lengths from seq len counts
# 	v <- rep.int(1:length(slc), times = c(slc))
# 	
# 	# and the nice algo for N50
# 	v.sorted <- rev(sort(v))
# 	return(list(
# 		sum_len = sum(v),
# 		nx = v.sorted[cumsum(as.numeric(v.sorted)) >= sum(as.numeric(v.sorted)) * n][1]
# 	))
# 	
# }

#=======================================================================
# start by executing fastqq on the input fqfiles, which are supplied as params by the calling script

# for parallelization of the fastqq call --> use mclapply
# # mclapply will not work on windows!
# if(Sys.info()[['sysname']] == "Windows") {
# 	qq <- lapply(params$fqfiles, seqTools::fastqq, k = 3)
# } else {
# 	cores <- parallel::detectCores()
# 	qq <- mclapply(params$fqfiles, seqTools::fastqq, 
# 								 k = 3, 
# 								 mc.cores = cores, 
# 								 mc.preschedule = FALSE) #works better on the workstation
# }

# # because fastqq does not error
# if(length(qq) == 0) {
# 	stop("No valid fastq file found")
# }
# 
# df <-	data.frame(
# 			file = basename(params$fqfiles),
# 			num_seqs = sapply(qq, seqTools::nReads),
# 			sum_len = sapply(1:length(qq), function(x) { get_nx(qq[[x]], 0.5)$sum_len } ), # total nucleotides
# 			min_len = sapply(qq, seqLen)[1, ],
# 			max_len = sapply(qq, seqLen)[2, ],
# 			n50 = sapply(1:length(qq), function(x) { get_nx(qq[[x]], 0.5)$nx } ),
# 			q20_percent = sapply(1:length(qq), function(x) { get_q(qq[[x]], 21) } ),
# 			q30_percent = sapply(1:length(qq), function(x) { get_q(qq[[x]], 31) } ),
# 			row.names = NULL
# 			)

# these files are published by the nxf script
write.csv(df, file = "fastq-stats.csv", row.names = FALSE)
write_xlsx(df, "fastq-stats.xlsx", format_headers = TRUE, col_names = TRUE)

```

***

### Number of reads and read quality metrics

```{r table1, include=TRUE}
num_seqs <- df %>% dplyr::summarise(seqs = sum(num_seqs)) %>% as.numeric()
num_bases <- df %>% dplyr::summarise(bases = sum(bases)) %>% as.numeric()
DT::datatable(df, 
							escape = F,
							filter = 'top', 
							caption = paste0("A total of ", nrow(df), " fastq files with ", 
															 format(num_seqs, big.mark = ","), 
															 " reads and ", 
															 format(num_bases, big.mark = ","), 
															 " bases."
															 ),
					#extensions = 'Buttons', 
					options = list(dom = 'Btp'
												 #buttons = c('copy', 'csv', 'excel')
												 ), 
					rownames = FALSE, 
					class = 'hover row-border') %>%
	DT::formatRound(2:7, 0) %>%
	DT::formatRound(8:9, 2)

```

***

### GC-content, Phred-score and k-mer distributions
```{r table2, include=TRUE}
# sparkline(0) # load dependencies
# see https://omnipotent.net/jquery.sparkline/#s-docs
# on how to include both x and y values in spark
# basically, supply values separated by a colon: x:y,x:y,x:y

faster_gc <- function(x) {
  system2("faster", args = c("--gc", x), stdout = TRUE) %>%
    as.numeric() %>%
    # actually use density() here, not hist(). It returns a density list object with x and y, x is fixed from 1 to 100
    density(from = 0, to = 1, n = 100, na.rm = TRUE) # n is the number of equally spaced points at which the density is to be estimated.
    #hist(plot = FALSE, breaks = c(0:100))
}

faster_qscore <- function(x) {
  system2("faster", args = c("--qscore", x), stdout = TRUE) %>%
    as.numeric() %>%
    # actually use density() here, not hist(). It returns a density list object with x and y, x is fixed from 1 to 50
    density(from = 1, to = 60, n = 60, na.rm = TRUE) # n is the number of equally spaced points at which the density is to be estimated.
  #
}

fastkmers <- function(x) {
	kmers <- system2("fastkmers", args = c("-k 3", "-v", x), stdout = TRUE)
	read.table(text = kmers, sep = "\t", header = TRUE, col.names = c("kmer", "counts")) %>%
		dplyr::arrange(kmer) # in order to compare across files
}

#---------------------------------#
# functions for making sparklines	#
#---------------------------------#

sparkline(0) # load dependencies

spk_tool <- function(label, x, values) {
   htmlwidgets::JS(
     sprintf(
 		"function(sparkline, options, field){ return %s[field[0].offset]; }",
     jsonlite::toJSON(paste(label, x, ":",values, sep = " "))
     )
   )
}

# see https://omnipotent.net/jquery.sparkline/#s-docs
# on how to include both x and y values in spark
# basically, supply values separated by a colon: x:y,x:y,x:y

spark_gc <- function(gc_density_obj) {
	spk_chr(paste( round(gc_density_obj$x, digits = 2), ":", gc_density_obj$y, sep = "" ), 
					lineWidth = 2,
					fillColor = "#D0D3D4",
					lineColor = "#5D6D7E",
					spotColor = FALSE,
					minSpotColor = FALSE,
					maxSpotColor = "red",
					spotRadius = 3,
					width = 220, height = 40,
					tooltipFormat = "<span style='color: {{color}}'>&#9679;</span> {{prefix}}avg GC% {{x}} {{suffix}}</span>"
					)
}

spark_phred <- function(phred_density_obj) {
	#spk_chr(paste( round(phred_density_obj$x, digits = 2), ":", phred_density_obj$y, sep = ""),
  fillcolor <- "#5D6D7E"
	spk_chr(round(phred_density_obj$y, digits = 2), 
					type = "bar",
					 # to highlight q-value of 30, only array (60 elements) seems to work, don't know how to pass range map here
					colorMap = c(rep(fillcolor, 19), "red", rep(fillcolor, 9), "red", rep(fillcolor, 30)),
					width = 220, height = 40,
					tooltipFormatter = spk_tool("qscore ",phred_density_obj$x, round(phred_density_obj$y, 2))
					)
}

spark_kmers <- function(kmers_tbl) {
  fillcolor <- "#5D6D7E"
	spk_chr(kmers_tbl$counts, type = "bar", 
	        barColor = "#5D6D7E",
	        #colorMap = c("red", rep(fillcolor, 15), "red", rep(fillcolor, 15), "red", rep(fillcolor, 15), "red", rep(fillcolor, 15)),
					width = 340, height = 40,
					tooltipFormatter = spk_tool("", kmers_tbl$kmer, kmers_tbl$counts)
					)
}

gc_density <- mclapply(1:length(fqfiles), function(y) { faster_gc(fqfiles[y]) })
q_score_density <- mclapply(1:length(fqfiles), function(y) { faster_qscore(fqfiles[y]) })
kmers_tbl_list <- mclapply(1:length(fqfiles), function(y) {fastkmers(fqfiles[y]) })

gc_df <- data.frame(
	file = basename(fqfiles),
	gc_content_dist = sapply(gc_density, spark_gc),
	#content_percycle = sapply(jsonfiles, spark_content),
	q_score_dist = sapply(q_score_density, spark_phred),
	kmer_counts = sapply(kmers_tbl_list, spark_kmers)
)

gc_df %>%
	dplyr::arrange(file) %>%
	kableExtra::kbl(escape = F, 
									caption = "Density distributions of GC-content, 'mean' q-score and k-mer counts (k = 3). The q-scores 20 and 30 are in red.") %>%
	kable_styling(fixed_thead = TRUE, bootstrap_options = c("responsive"))

```


